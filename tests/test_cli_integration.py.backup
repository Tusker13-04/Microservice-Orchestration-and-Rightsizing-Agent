#!/usr/bin/env python3
"""
CLI Integration Tests

End-to-end TDD tests for CLI workflows including:
- Training workflow via CLI
- Evaluation workflow via CLI
- Status checking
- Error handling
- Output validation
"""

import os
import sys
import pytest
import tempfile
import shutil
import pandas as pd
import numpy as np
from pathlib import Path
from unittest.mock import Mock, patch, MagicMock
import subprocess

# Add src to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

from click.testing import CliRunner
from src.mora.cli.main import main


class TestCLITrainingCommands:
    """Test CLI training commands"""
    
    def setup_method(self):
        """Set up test environment"""
        self.runner = CliRunner()
        self.temp_dir = tempfile.mkdtemp()
        self.data_dir = os.path.join(self.temp_dir, 'training_data')
        os.makedirs(self.data_dir, exist_ok=True)
        
        # Create sample training data
        self.create_sample_data()
        
        # Set up environment
        self.original_data_dir = os.environ.get('MORA_DATA_DIR', 'training_data')
        os.environ['MORA_DATA_DIR'] = self.data_dir
    
    def teardown_method(self):
        """Clean up test environment"""
        shutil.rmtree(self.temp_dir)
        os.environ['MORA_DATA_DIR'] = self.original_data_dir
    
    def create_sample_data(self):
        """Create sample training data"""
        data = {
            'timestamp': pd.date_range('2024-01-01', periods=50, freq='5min'),
            'cpu_cores_value': np.random.rand(50),
            'mem_bytes_value': np.random.rand(50) * 1000000,
            'net_rx_bytes_value': np.random.rand(50) * 100000,
            'net_tx_bytes_value': np.random.rand(50) * 100000,
            'pod_restarts_value': np.random.randint(0, 2, 50),
            'replica_count_value': np.random.randint(1, 5, 50),
            'load_users': np.random.randint(1, 100, 50)
        }
        
        df = pd.DataFrame(data)
        df.to_csv(
            os.path.join(self.data_dir, 'testservice_browsing_replicas_1_users_10.csv'),
            index=False
        )
    
    def test_train_lightweight_command_help(self):
        """Test lightweight training command help"""
        result = self.runner.invoke(main, ['train', 'lightweight', '--help'])
        
        assert result.exit_code == 0
        assert 'lightweight' in result.output.lower()
    
    def test_train_models_command_help(self):
        """Test professional training command help"""
        result = self.runner.invoke(main, ['train', 'models', '--help'])
        
        assert result.exit_code == 0
        assert 'models' in result.output.lower()
    
    def test_evaluate_command_help(self):
        """Test evaluation command help"""
        result = self.runner.invoke(main, ['train', 'evaluate', '--help'])
        
        assert result.exit_code == 0
        assert 'evaluate' in result.output.lower()
    
    def test_train_lightweight_with_service(self):
        """Test lightweight training with service specified"""
        result = self.runner.invoke(
            main,
            ['train', 'lightweight', '--service', 'testservice'],
            input='y\n'  # Confirm prompt
        )
        
        # Should either succeed or show appropriate error message
        assert result.exit_code in [0, 1, 2]
    
    def test_train_lightweight_no_service(self):
        """Test lightweight training without service specified"""
        result = self.runner.invoke(main, ['train', 'lightweight'])
        
        # Should show error or prompt for service
        assert result.exit_code != 0 or 'service' in result.output.lower()
    
    def test_train_lightweight_invalid_service(self):
        """Test lightweight training with invalid service"""
        result = self.runner.invoke(
            main,
            ['train', 'lightweight', '--service', 'nonexistent'],
            input='y\n'
        )
        
        # Should handle error gracefully
        assert result.exit_code != 0
    
    def test_train_models_with_service(self):
        """Test professional training with service specified"""
        result = self.runner.invoke(
            main,
            ['train', 'models', '--service', 'testservice'],
            input='y\n'
        )
        
        # Should either succeed or show appropriate error
        assert result.exit_code in [0, 1, 2]
    
    def test_evaluate_with_service(self):
        """Test evaluation with service specified"""
        result = self.runner.invoke(
            main,
            ['train', 'evaluate', '--service', 'testservice']
        )
        
        # Should handle gracefully (may fail if model doesn't exist)
        assert result.exit_code in [0, 1, 2]
    
    def test_evaluate_all_services(self):
        """Test evaluation of all services"""
        result = self.runner.invoke(main, ['train', 'evaluate', '--all'])
        
        # Should handle gracefully
        assert result.exit_code in [0, 1, 2]


class TestCLIStatusCommands:
    """Test CLI status commands"""
    
    def setup_method(self):
        """Set up test environment"""
        self.runner = CliRunner()
    
    def test_status_command(self):
        """Test status command"""
        result = self.runner.invoke(main, ['status'])
        
        # Status should run (may fail if Prometheus not running)
        assert result.exit_code in [0, 1, 2]
    
    def test_status_help(self):
        """Test status command help"""
        result = self.runner.invoke(main, ['status', '--help'])
        
        assert result.exit_code == 0
        assert 'status' in result.output.lower()
    
    def test_rightsize_command(self):
        """Test rightsize command"""
        result = self.runner.invoke(
            main,
            ['rightsize', '--service', 'frontend']
        )
        
        # Should handle gracefully (may fail if models don't exist)
        assert result.exit_code in [0, 1, 2]
    
    def test_rightsize_help(self):
        """Test rightsize command help"""
        result = self.runner.invoke(main, ['rightsize', '--help'])
        
        assert result.exit_code == 0
        assert 'rightsize' in result.output.lower()


class TestCLIErrorHandling:
    """Test CLI error handling"""
    
    def setup_method(self):
        """Set up test environment"""
        self.runner = CliRunner()
    
    def test_invalid_command(self):
        """Test handling of invalid command"""
        result = self.runner.invoke(main, ['invalid_command'])
        
        # Should show error message
        assert result.exit_code != 0
        assert 'error' in result.output.lower() or 'no such' in result.output.lower()
    
    def test_invalid_subcommand(self):
        """Test handling of invalid subcommand"""
        result = self.runner.invoke(main, ['train', 'invalid_subcommand'])
        
        # Should show error message
        assert result.exit_code != 0
    
    def test_missing_required_args(self):
        """Test handling of missing required arguments"""
        result = self.runner.invoke(main, ['train', 'lightweight'])
        
        # Should show error or prompt
        assert result.exit_code in [0, 1, 2]
    
    def test_conflicting_options(self):
        """Test handling of conflicting command options"""
        result = self.runner.invoke(
            main,
            ['train', 'evaluate', '--service', 'frontend', '--all']
        )
        
        # Should handle conflicting options
        assert result.exit_code in [0, 1, 2]


class TestCLIWorkflows:
    """Test complete CLI workflows"""
    
    def setup_method(self):
        """Set up test environment"""
        self.runner = CliRunner()
        self.temp_dir = tempfile.mkdtemp()
        self.data_dir = os.path.join(self.temp_dir, 'training_data')
        self.model_dir = os.path.join(self.temp_dir, 'models')
        os.makedirs(self.data_dir, exist_ok=True)
        os.makedirs(self.model_dir, exist_ok=True)
        
        # Create sample data
        self.create_sample_data()
        
        # Set environment variables
        self.original_data_dir = os.environ.get('MORA_DATA_DIR', 'training_data')
        self.original_model_dir = os.environ.get('MORA_MODEL_DIR', 'models')
        os.environ['MORA_DATA_DIR'] = self.data_dir
        os.environ['MORA_MODEL_DIR'] = self.model_dir
    
    def teardown_method(self):
        """Clean up test environment"""
        shutil.rmtree(self.temp_dir)
        os.environ['MORA_DATA_DIR'] = self.original_data_dir
        os.environ['MORA_MODEL_DIR'] = self.original_model_dir
    
    def create_sample_data(self):
        """Create sample training data"""
        data = {
            'timestamp': pd.date_range('2024-01-01', periods=100, freq='5min'),
            'cpu_cores_value': np.random.rand(100),
            'mem_bytes_value': np.random.rand(100) * 1000000,
            'net_rx_bytes_value': np.random.rand(100) * 100000,
            'net_tx_bytes_value': np.random.rand(100) * 100000,
            'pod_restarts_value': np.random.randint(0, 2, 100),
            'replica_count_value': np.random.randint(1, 5, 100),
            'load_users': np.random.randint(1, 100, 100)
        }
        
        df = pd.DataFrame(data)
        df.to_csv(
            os.path.join(self.data_dir, 'workflowtest_browsing_replicas_1_users_10.csv'),
            index=False
        )
    
    def test_complete_workflow(self):
        """Test complete training and evaluation workflow"""
        # This is a smoke test - just verify commands can be invoked
        service = 'workflowtest'
        
        # Step 1: Train
        result = self.runner.invoke(
            main,
            ['train', 'lightweight', '--service', service],
            input='y\n'
        )
        
        # Step 2: Evaluate
        result = self.runner.invoke(
            main,
            ['train', 'evaluate', '--service', service]
        )
        
        # Just verify commands can be invoked without crashing
        assert True  # If we get here, commands were invoked


class TestCLIOutputValidation:
    """Test CLI output formatting"""
    
    def setup_method(self):
        """Set up test environment"""
        self.runner = CliRunner()
    
    def test_help_output_formatting(self):
        """Test that help output is properly formatted"""
        result = self.runner.invoke(main, ['--help'])
        
        assert result.exit_code == 0
        assert 'MOrA' in result.output
        assert '\n' in result.output  # Should have multiple lines
    
    def test_command_output_structure(self):
        """Test that command outputs follow expected structure"""
        result = self.runner.invoke(main, ['status'])
        
        # Output should exist (may be error if Prometheus not running)
        assert isinstance(result.output, str)
        assert len(result.output) > 0 or result.exit_code != 0


if __name__ == '__main__':
    pytest.main([__file__, '-v'])

