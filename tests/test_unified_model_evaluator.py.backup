#!/usr/bin/env python3
"""
Unified Model Evaluator Tests

Comprehensive TDD tests for the unified evaluation system including:
- Model loading and validation
- Performance metric calculation
- Industry standards compliance
- Report generation
- Error handling
"""

import os
import sys
import pytest
import tempfile
import shutil
import pandas as pd
import numpy as np
from pathlib import Path
from unittest.mock import Mock, patch, MagicMock
import joblib

# Add src to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

from evaluate_models.unified_model_evaluator import UnifiedModelEvaluator


class TestUnifiedModelEvaluator:
    """Test the UnifiedModelEvaluator class"""
    
    def setup_method(self):
        """Set up test environment"""
        self.temp_dir = tempfile.mkdtemp()
        self.data_dir = os.path.join(self.temp_dir, 'training_data')
        self.model_dir = os.path.join(self.temp_dir, 'models')
        self.report_dir = os.path.join(self.temp_dir, 'evaluation_reports')
        os.makedirs(self.data_dir, exist_ok=True)
        os.makedirs(self.model_dir, exist_ok=True)
        os.makedirs(self.report_dir, exist_ok=True)
        
        # Create sample training data
        self.create_sample_data()
        
        # Create a mock trained model
        self.create_mock_model()
    
    def teardown_method(self):
        """Clean up test environment"""
        shutil.rmtree(self.temp_dir)
    
    def create_sample_data(self):
        """Create sample training data for testing"""
        data = {
            'timestamp': pd.date_range('2024-01-01', periods=100, freq='5min'),
            'cpu_cores_value': np.random.rand(100),
            'mem_bytes_value': np.random.rand(100) * 1000000,
            'net_rx_bytes_value': np.random.rand(100) * 100000,
            'net_tx_bytes_value': np.random.rand(100) * 100000,
            'pod_restarts_value': np.random.randint(0, 2, 100),
            'replica_count_value': np.random.randint(1, 5, 100),
            'load_users': np.random.randint(1, 100, 100)
        }
        
        df = pd.DataFrame(data)
        df.to_csv(
            os.path.join(self.data_dir, 'frontend_browsing_replicas_1_users_10.csv'),
            index=False
        )
    
    def create_mock_model(self):
        """Create a mock trained model for testing"""
        mock_pipeline = {
            'pipeline_type': 'lightweight_lstm_prophet',
            'trained_at': pd.Timestamp.now().isoformat(),
            'target_columns': ['cpu_target', 'memory_target', 'replica_target'],
            'feature_columns': ['cpu_cores_value', 'mem_bytes_value'],
            'data_shape': (100, 10),
            'scaler_X': Mock(),
            'scaler_y': Mock(),
            'prophet_models': {'cpu_target': Mock(), 'memory_target': Mock(), 'replica_target': Mock()},
            'lstm_models': {'cpu_target': Mock(), 'memory_target': Mock(), 'replica_target': Mock()}
        }
        
        joblib.dump(mock_pipeline, os.path.join(self.model_dir, 'frontend_lstm_prophet_pipeline.joblib'))
    
    def test_evaluator_initialization(self):
        """Test that evaluator initializes correctly"""
        evaluator = UnifiedModelEvaluator(
            model_dir=self.model_dir,
            data_dir=self.data_dir,
            report_dir=self.report_dir
        )
        
        assert evaluator.model_dir == self.model_dir
        assert evaluator.data_dir == self.data_dir
        assert evaluator.report_dir == self.report_dir
    
    def test_discover_models(self):
        """Test model discovery functionality"""
        evaluator = UnifiedModelEvaluator(
            model_dir=self.model_dir,
            data_dir=self.data_dir,
            report_dir=self.report_dir
        )
        
        models = evaluator._discover_models()
        
        assert len(models) > 0
        assert any('frontend' in m for m in models)
    
    def test_discover_models_no_files(self):
        """Test model discovery with no model files"""
        empty_model_dir = os.path.join(self.temp_dir, 'empty_models')
        os.makedirs(empty_model_dir, exist_ok=True)
        
        evaluator = UnifiedModelEvaluator(
            model_dir=empty_model_dir,
            data_dir=self.data_dir,
            report_dir=self.report_dir
        )
        
        with pytest.raises(ValueError, match="No trained models found"):
            evaluator._discover_models()
    
    def test_load_model(self):
        """Test loading a trained model"""
        evaluator = UnifiedModelEvaluator(
            model_dir=self.model_dir,
            data_dir=self.data_dir,
            report_dir=self.report_dir
        )
        
        model_path = os.path.join(self.model_dir, 'frontend_lstm_prophet_pipeline.joblib')
        model = evaluator._load_model(model_path)
        
        assert model is not None
        assert 'pipeline_type' in model
        assert 'trained_at' in model
    
    def test_load_model_invalid_path(self):
        """Test loading a non-existent model"""
        evaluator = UnifiedModelEvaluator(
            model_dir=self.model_dir,
            data_dir=self.data_dir,
            report_dir=self.report_dir
        )
        
        with pytest.raises(FileNotFoundError):
            evaluator._load_model('non_existent_model.joblib')
    
    def test_load_model_corrupted_file(self):
        """Test loading a corrupted model file"""
        # Create a corrupted file
        bad_file = os.path.join(self.model_dir, 'bad_model.joblib')
        with open(bad_file, 'w') as f:
            f.write("not a valid joblib file\n")
        
        evaluator = UnifiedModelEvaluator(
            model_dir=self.model_dir,
            data_dir=self.data_dir,
            report_dir=self.report_dir
        )
        
        with pytest.raises((ValueError, EOFError)):
            evaluator._load_model('bad_model.joblib')
    
    def test_load_test_data(self):
        """Test loading test data for evaluation"""
        evaluator = UnifiedModelEvaluator(
            model_dir=self.model_dir,
            data_dir=self.data_dir,
            report_dir=self.report_dir
        )
        
        data = evaluator._load_test_data('frontend')
        
        assert isinstance(data, pd.DataFrame)
        assert len(data) > 0
    
    def test_load_test_data_no_files(self):
        """Test loading test data when no files exist"""
        empty_data_dir = os.path.join(self.temp_dir, 'empty_data')
        os.makedirs(empty_data_dir, exist_ok=True)
        
        evaluator = UnifiedModelEvaluator(
            model_dir=self.model_dir,
            data_dir=empty_data_dir,
            report_dir=self.report_dir
        )
        
        with pytest.raises(ValueError, match="No test data files found"):
            evaluator._load_test_data('frontend')
    
    def test_calculate_performance_metrics(self):
        """Test performance metric calculation"""
        evaluator = UnifiedModelEvaluator(
            model_dir=self.model_dir,
            data_dir=self.data_dir,
            report_dir=self.report_dir
        )
        
        # Create mock predictions
        y_true = pd.DataFrame({
            'cpu_target': [0.5, 0.6, 0.4],
            'memory_target': [1000000, 1200000, 900000],
            'replica_target': [2, 3, 2]
        })
        
        y_pred = pd.DataFrame({
            'cpu_target': [0.55, 0.58, 0.45],
            'memory_target': [1100000, 1180000, 950000],
            'replica_target': [2, 3, 2]
        })
        
        metrics = evaluator._calculate_metrics(y_true, y_pred)
        
        assert 'cpu_target' in metrics
        assert 'memory_target' in metrics
        assert 'replica_target' in metrics
    
    def test_check_industry_compliance(self):
        """Test industry compliance checking"""
        evaluator = UnifiedModelEvaluator(
            model_dir=self.model_dir,
            data_dir=self.data_dir,
            report_dir=self.report_dir
        )
        
        # Mock metrics
        metrics = {
            'cpu_target': {
                'mse': 0.001,
                'mae': 0.01,
                'r2': 0.8,
                'confidence': 0.85
            }
        }
        
        compliance = evaluator._check_industry_compliance('cpu_target', metrics['cpu_target'])
        
        assert 'mse_compliant' in compliance
        assert 'mae_compliant' in compliance
        assert 'r2_compliant' in compliance
    
    def test_generate_evaluation_report(self):
        """Test report generation"""
        evaluator = UnifiedModelEvaluator(
            model_dir=self.model_dir,
            data_dir=self.data_dir,
            report_dir=self.report_dir
        )
        
        # Mock model and metrics
        model = {
            'pipeline_type': 'lightweight_lstm_prophet',
            'trained_at': '2024-01-01T00:00:00'
        }
        
        metrics = {
            'cpu_target': {
                'mse': 0.001,
                'mae': 0.01,
                'r2': 0.8,
                'confidence': 0.85
            }
        }
        
        compliance = {
            'cpu_target': {
                'mse_compliant': True,
                'mae_compliant': True,
                'r2_compliant': True,
                'overall_compliant': True
            }
        }
        
        report_path = evaluator._generate_report('frontend', model, metrics, compliance)
        
        assert os.path.exists(report_path)
        assert os.path.basename(report_path).startswith('frontend_evaluation')
    
    def test_evaluate_single_service(self):
        """Test evaluating a single service"""
        evaluator = UnifiedModelEvaluator(
            model_dir=self.model_dir,
            data_dir=self.data_dir,
            report_dir=self.report_dir
        )
        
        evaluator.evaluate_service('frontend')
        
        # Check that report was generated
        reports = [f for f in os.listdir(self.report_dir) if f.endswith('.txt')]
        assert len(reports) > 0
        assert any('frontend' in r for r in reports)
    
    def test_evaluate_service_no_model(self):
        """Test evaluating a service with no trained model"""
        evaluator = UnifiedModelEvaluator(
            model_dir=self.model_dir,
            data_dir=self.data_dir,
            report_dir=self.report_dir
        )
        
        with pytest.raises(FileNotFoundError):
            evaluator.evaluate_service('non_existent_service')
    
    def test_calculate_overall_score(self):
        """Test calculating overall evaluation score"""
        evaluator = UnifiedModelEvaluator(
            model_dir=self.model_dir,
            data_dir=self.data_dir,
            report_dir=self.report_dir
        )
        
        compliance = {
            'cpu_target': {
                'mse_compliant': True,
                'mae_compliant': True,
                'r2_compliant': False,
                'overall_compliant': False,
                'confidence_compliant': True
            },
            'memory_target': {
                'mse_compliant': True,
                'mae_compliant': True,
                'r2_compliant': True,
                'overall_compliant': True,
                'confidence_compliant': True
            },
            'replica_target': {
                'mse_compliant': True,
                'mae_compliant': True,
                'r2_compliant': True,
                'overall_compliant': True,
                'confidence_compliant': True
            }
        }
        
        score = evaluator._calculate_overall_score(compliance)
        
        assert 0 <= score <= 100
        assert isinstance(score, (int, float))


class TestUnifiedEvaluatorIntegration:
    """Integration tests for complete evaluation workflows"""
    
    def setup_method(self):
        """Set up test environment"""
        self.temp_dir = tempfile.mkdtemp()
        self.data_dir = os.path.join(self.temp_dir, 'training_data')
        self.model_dir = os.path.join(self.temp_dir, 'models')
        self.report_dir = os.path.join(self.temp_dir, 'evaluation_reports')
        os.makedirs(self.data_dir, exist_ok=True)
        os.makedirs(self.model_dir, exist_ok=True)
        os.makedirs(self.report_dir, exist_ok=True)
        
        # Create comprehensive sample data
        self.create_sample_data()
        
        # Create multiple mock models
        self.create_mock_models()
    
    def teardown_method(self):
        """Clean up test environment"""
        shutil.rmtree(self.temp_dir)
    
    def create_sample_data(self):
        """Create sample data for multiple services"""
        services = ['frontend', 'cartservice']
        
        for service in services:
            data = {
                'timestamp': pd.date_range('2024-01-01', periods=100, freq='5min'),
                'cpu_cores_value': np.random.rand(100),
                'mem_bytes_value': np.random.rand(100) * 1000000,
                'net_rx_bytes_value': np.random.rand(100) * 100000,
                'net_tx_bytes_value': np.random.rand(100) * 100000,
                'pod_restarts_value': np.random.randint(0, 2, 100),
                'replica_count_value': np.random.randint(1, 5, 100),
                'load_users': np.random.randint(1, 100, 100)
            }
            
            df = pd.DataFrame(data)
            df.to_csv(
                os.path.join(self.data_dir, f'{service}_browsing_replicas_1_users_10.csv'),
                index=False
            )
    
    def create_mock_models(self):
        """Create mock models for multiple services"""
        for service in ['frontend', 'cartservice']:
            mock_pipeline = {
                'pipeline_type': 'lightweight_lstm_prophet',
                'trained_at': pd.Timestamp.now().isoformat(),
                'target_columns': ['cpu_target', 'memory_target', 'replica_target'],
                'feature_columns': ['cpu_cores_value', 'mem_bytes_value'],
                'data_shape': (100, 10),
                'scaler_X': Mock(),
                'scaler_y': Mock(),
                'prophet_models': {'cpu_target': Mock(), 'memory_target': Mock(), 'replica_target': Mock()},
                'lstm_models': {'cpu_target': Mock(), 'memory_target': Mock(), 'replica_target': Mock()}
            }
            
            joblib.dump(
                mock_pipeline,
                os.path.join(self.model_dir, f'{service}_lstm_prophet_pipeline.joblib')
            )
    
    def test_evaluate_multiple_services(self):
        """Test evaluating multiple services"""
        evaluator = UnifiedModelEvaluator(
            model_dir=self.model_dir,
            data_dir=self.data_dir,
            report_dir=self.report_dir
        )
        
        services = ['frontend', 'cartservice']
        
        for service in services:
            evaluator.evaluate_service(service)
        
        # Check that reports were generated
        reports = [f for f in os.listdir(self.report_dir) if f.endswith('.txt')]
        assert len(reports) >= len(services)
    
    def test_evaluate_all_services(self):
        """Test evaluating all available services"""
        evaluator = UnifiedModelEvaluator(
            model_dir=self.model_dir,
            data_dir=self.data_dir,
            report_dir=self.report_dir
        )
        
        evaluator.evaluate_all()
        
        # Check that reports were generated
        reports = [f for f in os.listdir(self.report_dir) if f.endswith('.txt')]
        assert len(reports) >= 2  # At least frontend and cartservice


if __name__ == '__main__':
    pytest.main([__file__, '-v'])

